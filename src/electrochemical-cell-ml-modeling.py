# -*- coding: utf-8 -*-
"""dk3202_Final_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JzPJ1VAupqgBWJih68tpyG4-cCYhxXNx

## 1: Loading Data
"""

from google.colab import drive
drive.mount('/content/gdrive')

import os
import numpy as np
import matplotlib.pyplot as plt
os.chdir('/content/gdrive/MyDrive/final_project')

f = open('Features_Description_15.txt', 'r')
featuresDescription = f.read()
print (featuresDescription)
f.close()

inputData = np.load('INPUT_DATA_15.npy', encoding='bytes')
targetData = np.load('TARGET_DATA_15.npy', encoding='bytes')

print(inputData.shape)
print(targetData.shape)

# datasets shapes, number of features, number of examples, targets
print('Dataset shape is: {}'.format(inputData.shape))
print('Number of examples: {}'.format(inputData.shape[0]))
print('Number of features: {}'.format(inputData.shape[1]))
print('Number of target examples: {}'.format(targetData.shape[0]))

import seaborn as sns
import pandas as pd
df= pd.DataFrame(inputData)
df['Target'] = pd.DataFrame(targetData)
allData = df.to_numpy()

df = pd.DataFrame(allData,
             columns=['CRATE', 'RXNK', 'LENGTH', 'EFF_TORTUOSITY', 'DIFF_BULK', 'POROSITY',
 'DIFF_C_BULK', 'CONDUCTIVITY', 'CAPACITY'])

df.head()

plt.rc('font', size=15)  # set the default font size
plt.rc('figure', figsize=(10,7))  # set the default figure size

"""## 2: Visualizing Data"""

# Checking for missing values in the dataset

miss_vals_count = df.isnull().sum()
miss_vals_count[0:9]

# Check outliers
plt.subplots(nrows=3,ncols=3, figsize=(16,9))  # define subplot grid with nrows and ncols

plt.suptitle('The 9 features for the input data on the dataset')

plt.subplot(3,3,1)
sns.boxplot(df['CRATE'])

plt.subplot(3,3,2)
sns.boxplot(df['RXNK'])

plt.subplot(3,3, 3)
sns.boxplot(df['LENGTH'])

plt.subplot(3,3,4)
sns.boxplot(df['EFF_TORTUOSITY'])

plt.subplot(3,3,5)
sns.boxplot(df['DIFF_BULK'])

plt.subplot(3,3,6)
sns.boxplot(df['POROSITY'])

plt.subplot(3,3,7)
sns.boxplot(df['DIFF_C_BULK'])

plt.subplot(3,3,8)
sns.boxplot(df['CONDUCTIVITY'])

plt.subplot(3,3,9)
sns.boxplot(df['CAPACITY'])

print(df['CAPACITY'].describe())
plt.figure(figsize=(9, 8))
sns.distplot(df['CAPACITY'], color='b', bins=100, hist_kws={'alpha': 0.4})
plt.style.use('bmh')

# plot distribution of target variables
targ_vals, targ_freq = np.unique(targetData, return_counts=True)

plt.bar(x=targ_vals, height=targ_freq, alpha=0.75, color='green')
plt.xlabel('x')
plt.ylabel('y')
plt.title('Distribution of x and y');

# subplots to visualize some features

plt.subplots(nrows=3,ncols=3, figsize=(16,9))  # define subplot grid with nrows and ncols

plt.suptitle('The 9 features for the input data on the dataset')

plt.subplot(3,3,1)
plt.plot(allData[:,0])

plt.subplot(3,3,2)
plt.plot(allData[:,1])

plt.subplot(3,3, 3)
plt.plot(allData[:,2])

plt.subplot(3,3,4)
plt.plot(allData[:,3])

plt.subplot(3,3,5)
plt.plot(allData[:,4])

plt.subplot(3,3,6)
plt.plot(allData[:,5])

plt.subplot(3,3,7)
plt.plot(allData[:,6])

plt.subplot(3,3,8)
plt.plot(allData[:,7])

plt.subplot(3,3,9)
plt.plot(allData[:,8])

# plot all of them together!! 9x9 grid; 81 plots
sns.pairplot(df)

plt.style.use('default')     # switches back to matplotlib style

df.corr()

# lets create a correlation headmap to see if it is worth it to use PCA
plt.figure(figsize=(16, 6))
# Set the range of values to be displayed on the colormap from -1 to 1, and set the annotation to True to display the correlation values on the heatmap.
heatmap = sns.heatmap(df.corr(), vmin=-1, vmax=1, annot=True)
# Give a title to the heatmap. Pad defines the distance of the title from the top of the heatmap.
heatmap.set_title('Correlation Heatmap', fontdict={'fontsize':12}, pad=12);

"""## 3: Data Preprocessing"""

from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.metrics import accuracy_score
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier

# Mean-centering data
df_centered = df.apply(lambda x: x-x.mean())
df_centered.head()
print(df_centered['CAPACITY'].describe())
plt.figure(figsize=(9, 8))
sns.distplot(df_centered['CAPACITY'], color='g', bins=100, hist_kws={'alpha': 0.4})
plt.style.use('bmh')
df_centered.hist(figsize=(16, 20), bins=50, xlabelsize=8, ylabelsize=8)

# df_centered.corr()
plt.figure(figsize=(16, 6))
heatmap = sns.heatmap(df_centered.corr(), vmin=-1, vmax=1, annot=True)
heatmap.set_title('Correlation Heatmap', fontdict={'fontsize':12}, pad=12);

sd = np.std(df, axis=0)
df_new=df_centered/sd
df_new.head()
CovMat = (df_new.T@df_new)/(df_new.shape[0]-1)  # Covariance formula
print(CovMat)

eigval, eigvec = np.linalg.eig(CovMat)  # linalg library in Python for linear lagebra.
print('Eigevalues: {}'.format(eigval))
print('First eigenvector: {}'.format(eigvec[:, 0]))
print('Second eigenvector: {}'.format(eigvec[:, 1]))

X = df_new.drop(columns='CAPACITY', axis=1)
Y = df_new['CAPACITY']
print(X)
print(Y)

"""##Train Test Split (80/20)"""

from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import StandardScaler

cols = [0,1,2,3,4,5,6,7]
X = df_centered[df_centered.columns[cols]].values
y = df_centered['CAPACITY'].values

# X_train, y_train, X_valid, y_valid, X_test, y_test = train_valid_test_split(df, target = 'Target',
#                                                                             train_size=0.70, valid_size=0.15, test_size=0.15)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)
print(X.shape); print(X_train.shape); print(X_test.shape)

scaler = StandardScaler()
#scaler = MinMaxScaler()
scaler.fit(X_train)
X_train_scaled = scaler.transform(X_train)

X_test_scaled = scaler.transform(X_test)

"""## 4: Modeling"""

import numpy as np
from sklearn import linear_model
from sklearn import svm
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score
from sklearn.metrics import mean_squared_error
from math import sqrt
from sklearn.ensemble import HistGradientBoostingRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.linear_model import Ridge
from sklearn.linear_model import ElasticNet

classifiers = [
    svm.SVR(),
    linear_model.SGDRegressor(),
    linear_model.BayesianRidge(),
    linear_model.ARDRegression(),
    linear_model.LinearRegression(),
    Ridge(alpha=0.01),
    ElasticNet(alpha = 0.01),
    linear_model.TheilSenRegressor(),
    HistGradientBoostingRegressor(),
    RandomForestRegressor(max_depth=100, random_state=0),
    GradientBoostingRegressor(random_state=0)]

for item in classifiers:
  print('\n')
  print(item)
  clf = item
  clf.fit(X_train_scaled, y_train)
  pred_train_clf= clf.predict(X_train_scaled)
  print('Train MSE: ', np.sqrt(mean_squared_error(y_train,pred_train_clf)))
  print('Train R2 Score: ', r2_score(y_train, pred_train_clf))
  pred_test_clf= clf.predict(X_test_scaled)
  print('Test MSE: ', np.sqrt(mean_squared_error(y_test,pred_test_clf)))
  print('Test Score: ', r2_score(y_test, pred_test_clf))

import numpy as np
from sklearn import linear_model
from sklearn import svm
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score
from sklearn.metrics import mean_squared_error
from math import sqrt
from sklearn.ensemble import HistGradientBoostingRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import Ridge
from sklearn.linear_model import ElasticNet
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler

classifiers = [
    svm.SVR(),
    linear_model.SGDRegressor(),
    linear_model.BayesianRidge(),
    linear_model.LassoLars(),
    linear_model.ARDRegression(),
    linear_model.PassiveAggressiveRegressor(),
    linear_model.LinearRegression(),
    Ridge(alpha=0.01),
    ElasticNet(alpha = 0.01),
    linear_model.TheilSenRegressor(),
    linear_model.Lasso(),
    HistGradientBoostingRegressor(),
    RandomForestRegressor(max_depth=100, random_state=0),
    GradientBoostingRegressor(random_state=0)
    ]


for item in classifiers:
  print('Model: ', item)
  cv_score_item = cross_val_score(item, X, Y, cv = 5)
  mean_accuracy_item = sum(cv_score_item)/len(cv_score_item)
  mean_accuracy_item = mean_accuracy_item*100
  mean_accuracy_item = round(mean_accuracy_item, 2)
  print(mean_accuracy_item)

"""## Hyperparameter tunning"""

# Commented out IPython magic to ensure Python compatibility.
from sklearn.model_selection import GridSearchCV

max_features_range = np.arange(1,6,1)
n_estimators_range = np.arange(10,210,10)
param_grid = dict(max_features=max_features_range, n_estimators=n_estimators_range)

rf = GradientBoostingRegressor()

grid = GridSearchCV(estimator=rf, param_grid=param_grid, cv=2)
grid.fit(X_train, y_train)

print("The best parameters are %s with a score of %0.2f"
#       % (grid.best_params_, grid.best_score_))

# Commented out IPython magic to ensure Python compatibility.
from sklearn.model_selection import GridSearchCV

max_features_range = np.arange(1,400,100)
random_state_range = np.arange(0,100,10)
n_estimators_range = np.arange(10,210,10)
param_grid = dict(max_features=max_features_range,
                  n_estimators=n_estimators_range)

rf = RandomForestRegressor()

grid = GridSearchCV(estimator=rf, param_grid=param_grid, cv=2)
grid.fit(X_train, y_train)

print("The best parameters are %s with a score of %0.2f"
#       % (grid.best_params_, grid.best_score_))

# Commented out IPython magic to ensure Python compatibility.
from sklearn.model_selection import GridSearchCV

param_grid = {'C': [0.1, 1, 10, 100, 1000, 10000],
              'epsilon': [0.0001, 0.001, 0.01, 0.01,1,10]}

rf = svm.SVR()

grid = GridSearchCV(estimator=rf, param_grid=param_grid, refit = True, verbose = 0)
grid.fit(X_train, y_train)

print("The best parameters are %s with a score of %0.2f"
#       % (grid.best_params_, grid.best_score_))

# Commented out IPython magic to ensure Python compatibility.
from sklearn.model_selection import GridSearchCV

max_features_range = np.arange(1,6,1)
n_estimators_range = np.arange(10,210,10)
param_grid = dict(max_iter=max_features_range, random_state=n_estimators_range)

rf = HistGradientBoostingRegressor()

grid = GridSearchCV(estimator=rf, param_grid=param_grid, cv=2)
grid.fit(X_train, y_train)

print("The best parameters are %s with a score of %0.2f"
#       % (grid.best_params_, grid.best_score_))

"""## Running models with hyperparameters"""

import numpy as np
from sklearn import linear_model
from sklearn import svm
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score
from sklearn.metrics import mean_squared_error
from math import sqrt
from sklearn.ensemble import HistGradientBoostingRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.linear_model import Ridge
from sklearn.linear_model import ElasticNet

classifiers = [
    svm.SVR(C=10000, epsilon=10),
    HistGradientBoostingRegressor(max_iter=5, random_state=10),
    RandomForestRegressor(max_depth=101, max_features=1),
    GradientBoostingRegressor(max_features=5, n_estimators=180)]

for item in classifiers:
  print('\n')
  print(item)
  clf = item
  clf.fit(X_train_scaled, y_train)
  pred_train_clf= clf.predict(X_train_scaled)
  print('Train MSE: ', np.sqrt(mean_squared_error(y_train,pred_train_clf)))
  print('Train R2 Score: ', r2_score(y_train, pred_train_clf))
  pred_test_clf= clf.predict(X_test_scaled)
  print('Test MSE: ', np.sqrt(mean_squared_error(y_test,pred_test_clf)))
  print('Test Score: ', r2_score(y_test, pred_test_clf))

import numpy as np
from sklearn import linear_model
from sklearn import svm
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score
from sklearn.metrics import mean_squared_error
from math import sqrt
from sklearn.ensemble import HistGradientBoostingRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import Ridge
from sklearn.linear_model import ElasticNet
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler

classifiers = [
    svm.SVR(C=10000, epsilon=10),
    HistGradientBoostingRegressor(max_iter=5, random_state=10),
    RandomForestRegressor(max_depth=101, max_features=1),
    GradientBoostingRegressor(max_features=5, n_estimators=180)]


for item in classifiers:
  print('Model: ', item)
  cv_score_item = cross_val_score(item, X, Y, cv = 5)
  mean_accuracy_item = sum(cv_score_item)/len(cv_score_item)
  mean_accuracy_item = mean_accuracy_item*100
  mean_accuracy_item = round(mean_accuracy_item, 2)
  print(mean_accuracy_item)

"""## SHAPLEY Values"""

!pip install shap
import shap
from sklearn import tree
model = svm.SVR(C=10000, epsilon=10).fit(X_train, y_train)
explainer = shap.KernelExplainer(model.predict, X_train)
shap_values = explainer.shap_values(X_test)

shap.summary_plot(shap_values, X_test, plot_type='bar')

model = HistGradientBoostingRegressor(max_iter=5, random_state=10).fit(X_train, y_train)
explainer = shap.KernelExplainer(model.predict, X_train)
shap_values = explainer.shap_values(X_test)

shap.summary_plot(shap_values, X_test, plot_type='bar')

model = RandomForestRegressor(max_depth=101, max_features=1, n_estimators=120, random_state=70).fit(X_train, y_train)
explainer = shap.KernelExplainer(model.predict, X_train)
shap_values = explainer.shap_values(X_test)

shap.summary_plot(shap_values, X_test, plot_type='bar')

model = GradientBoostingRegressor(max_features=5, n_estimators=180).fit(X_train, y_train)
explainer = shap.KernelExplainer(model.predict, X_train)
shap_values = explainer.shap_values(X_test)

shap.summary_plot(shap_values, X_test, plot_type='bar')

"""# In contrast and comparison

- Running PCA and then the models
"""

from sklearn.decomposition import PCA
type(inputData)
Xpca= df

mu = np.mean(Xpca, axis=0) # mean vector
print('Mean: {}'.format(mu))

Xmu = Xpca - mu  # mean-centered

sd = np.std(Xpca, axis=0)
print('Std Dev: {}'.format(sd))
Xmustd = Xmu/sd  # normalized by std. dev.

pca=PCA(n_components=9)

pca.fit(Xmustd)

pca.singular_values_

pca.explained_variance_ratio_

pca.explained_variance_ratio_.cumsum()

PCs = pca.fit_transform(Xmustd)

df_PCs = pd.DataFrame(PCs)

df_PCs

sns.pairplot(df_PCs)  # none of the features (PCs) are correlated --> orthogonal features!

plt.style.use('default')

df_PCs.corr()

df_PCs.plot.scatter(x=0, y=8)
df_PCs.plot.scatter(x=1, y=8)
df_PCs.plot.scatter(x=2, y=8)
df_PCs.plot.scatter(x=3, y=8)
df_PCs.plot.scatter(x=4, y=8)
df_PCs.plot.scatter(x=5, y=8)
df_PCs.plot.scatter(x=6, y=8)
df_PCs.plot.scatter(x=7, y=8)

from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import StandardScaler

cols = [0,1,2,3,4,5,6,7]
Xpca = df_PCs.values
ypca = df_PCs[8].values

X_train, X_test, y_train, y_test = train_test_split(Xpca, ypca, test_size=0.20, random_state=42)
print(X.shape); print(X_train.shape); print(X_test.shape)

scaler = StandardScaler()
scaler.fit(X_train)
X_train_scaled = scaler.transform(X_train)

X_test_scaled = scaler.transform(X_test)

import numpy as np
from sklearn import linear_model
from sklearn import svm
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score
from sklearn.metrics import mean_squared_error
from math import sqrt
from sklearn.ensemble import HistGradientBoostingRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.linear_model import Ridge
from sklearn.linear_model import ElasticNet

classifiers = [
    svm.SVR(),
    linear_model.SGDRegressor(),
    linear_model.BayesianRidge(),
    linear_model.ARDRegression(),
    linear_model.LinearRegression(),
    Ridge(alpha=0.01),
    ElasticNet(alpha = 0.01),
    linear_model.TheilSenRegressor(),
    HistGradientBoostingRegressor(),
    RandomForestRegressor(max_depth=100, random_state=0),
    GradientBoostingRegressor(random_state=0)]

for item in classifiers:
  print('\n')
  print(item)
  clf = item
  clf.fit(X_train_scaled, y_train)
  pred_train_clf= clf.predict(X_train_scaled)
  print('Train MSE: ', np.sqrt(mean_squared_error(y_train,pred_train_clf)))
  print('Train R2 Score: ', r2_score(y_train, pred_train_clf))
  pred_test_clf= clf.predict(X_test_scaled)
  print('Test MSE: ', np.sqrt(mean_squared_error(y_test,pred_test_clf)))
  print('Test Score: ', r2_score(y_test, pred_test_clf))

import numpy as np
from sklearn import linear_model
from sklearn import svm
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score
from sklearn.metrics import mean_squared_error
from math import sqrt
from sklearn.ensemble import HistGradientBoostingRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import Ridge
from sklearn.linear_model import ElasticNet
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler

classifiers = [
    svm.SVR(),
    linear_model.SGDRegressor(),
    linear_model.BayesianRidge(),
    linear_model.LassoLars(),
    linear_model.ARDRegression(),
    linear_model.PassiveAggressiveRegressor(),
    linear_model.LinearRegression(),
    Ridge(alpha=0.01),
    ElasticNet(alpha = 0.01),
    linear_model.TheilSenRegressor(),
    linear_model.Lasso(),
    HistGradientBoostingRegressor(),
    RandomForestRegressor(max_depth=100, random_state=0),
    GradientBoostingRegressor(random_state=0)
    ]


for item in classifiers:
  print('Model: ', item)
  cv_score_item = cross_val_score(item, Xpca, ypca, cv = 5)
  mean_accuracy_item = sum(cv_score_item)/len(cv_score_item)
  mean_accuracy_item = mean_accuracy_item*100
  mean_accuracy_item = round(mean_accuracy_item, 2)
  print(mean_accuracy_item)